{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-11-23T01:44:11.325020Z","iopub.status.busy":"2024-11-23T01:44:11.324229Z","iopub.status.idle":"2024-11-23T02:49:56.009155Z","shell.execute_reply":"2024-11-23T02:49:56.008170Z","shell.execute_reply.started":"2024-11-23T01:44:11.324989Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/lavis/models/base_model.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n"]},{"name":"stdout","output_type":"stream","text":["Updated CSV file saved at: output_vqa_answers.csv\n"]}],"source":["import os\n","import pandas as pd\n","from PIL import Image\n","import torch\n","from lavis.models import load_model_and_preprocess\n","\n","# Load LAVIS VQA model and preprocessors\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, vis_processors, _ = load_model_and_preprocess(\n","    name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=device\n",")\n","\n","# Define VQA questions\n","questions = [\n","    \"What is the race of the person in the image?\",\n","    \"What is the gender of the person in the image?\",\n","    \"What is the religion of the person in the image?\",\n","    \"Which country does the person in the image come from?\",\n","    \"Are there disabled people in the image?\",\n","    \"What animal is in the image?\",\n","    \"Is there a person in the image?\",\n","    \"Is there an animal in the image?\",\n","]\n","\n","# Define a generic answer list\n","answer_list = [\n","    \"Caucasian\", \"Asian\", \"African\", \"Male\", \"Female\", \"Christianity\", \"Islam\", \"Judaism\",\n","    \"USA\", \"India\", \"China\", \"Yes\", \"No\", \"Dog\", \"Cat\", \"Bird\", \"Person\", \"Animal\"\n","]\n","\n","# Load CSV file\n","csv_path = \"/kaggle/input/labelandcap/merged_data.csv\"\n","output_csv_path = \"output_vqa_answers.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# Image folder path\n","image_folder = \"/kaggle/input/requirement/resources/resources/datasets/harmeme/img\"\n","\n","# Generate VQA answers\n","answers = []\n","\n","for index, row in df.iterrows():\n","    image_filename = row['image']\n","    image_path = os.path.join(image_folder, image_filename)\n","\n","    if not os.path.exists(image_path):\n","        print(f\"Image not found: {image_path}\")\n","        answers.append({question: \"Image Not Found\" for question in questions})\n","        continue\n","\n","    try:\n","        # Load image\n","        image = Image.open(image_path).convert(\"RGB\")\n","        processed_image = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n","\n","        # Generate answers for each question\n","        answer_dict = {}\n","        for question in questions:\n","            inputs = {\"image\": processed_image, \"text_input\": question}\n","            answer = model.predict_answers(\n","                samples=inputs, num_beams=5, num_answers=1, answer_list=answer_list\n","            )\n","            answer_dict[question] = answer[0]  # Top answer\n","        answers.append(answer_dict)\n","\n","    except Exception as e:\n","        print(f\"Error processing image {image_filename}: {e}\")\n","        answers.append({question: \"Error Processing Image\" for question in questions})\n","\n","# Merge VQA answers with the original DataFrame\n","answers_df = pd.DataFrame(answers)\n","df = pd.concat([df, answers_df], axis=1)\n","\n","# Save updated DataFrame to a new CSV file\n","df.to_csv(output_csv_path, index=False)\n","print(f\"Updated CSV file saved at: {output_csv_path}\")"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T04:37:21.265850Z","iopub.status.busy":"2024-11-24T04:37:21.265529Z","iopub.status.idle":"2024-11-24T04:37:35.795775Z","shell.execute_reply":"2024-11-24T04:37:35.794749Z","shell.execute_reply.started":"2024-11-24T04:37:21.265821Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-e86qe0iy\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-e86qe0iy\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\n","Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=f282ced5a0020855e975f65900b4d5c064a7b32e8a5688ed8ef01a7c17688017\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5mpl4kb9/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.3.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T04:39:35.295276Z","iopub.status.busy":"2024-11-24T04:39:35.294953Z","iopub.status.idle":"2024-11-24T04:39:44.813735Z","shell.execute_reply":"2024-11-24T04:39:44.812530Z","shell.execute_reply.started":"2024-11-24T04:39:35.295247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.6.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n","Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\n","Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.8.30)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-geometric\n","Successfully installed torch-geometric-2.6.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu118.html"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T05:50:51.476845Z","iopub.status.busy":"2024-11-24T05:50:51.476492Z","iopub.status.idle":"2024-11-24T05:50:52.845765Z","shell.execute_reply":"2024-11-24T05:50:52.844921Z","shell.execute_reply.started":"2024-11-24T05:50:51.476814Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/198207019.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  embeddings = torch.load(embedding_path)\n"]},{"name":"stdout","output_type":"stream","text":["Train embeddings: 3013\n","Validation embeddings: 177\n","Test embeddings: 354\n"]}],"source":["import torch\n","import pandas as pd\n","\n","csv_path = \"/kaggle/input/with-vqa-data/output_vqa_answers (2).csv\"\n","df = pd.read_csv(csv_path)\n","\n","embedding_path = \"/kaggle/input/combine-embedding/combined_embeddings.pt\"\n","embeddings = torch.load(embedding_path)\n","\n","train_df = df[df[\"split\"] == \"train\"]\n","val_df = df[df[\"split\"] == \"val\"]\n","test_df = df[df[\"split\"] == \"test\"]\n","\n","train_embeddings = [e for e in embeddings if e[\"image\"] in train_df[\"image\"].values]\n","val_embeddings = [e for e in embeddings if e[\"image\"] in val_df[\"image\"].values]\n","test_embeddings = [e for e in embeddings if e[\"image\"] in test_df[\"image\"].values]\n","\n","print(f\"Train embeddings: {len(train_embeddings)}\")\n","print(f\"Validation embeddings: {len(val_embeddings)}\")\n","print(f\"Test embeddings: {len(test_embeddings)}\")"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T05:51:17.177323Z","iopub.status.busy":"2024-11-24T05:51:17.176936Z","iopub.status.idle":"2024-11-24T05:51:17.847146Z","shell.execute_reply":"2024-11-24T05:51:17.846057Z","shell.execute_reply.started":"2024-11-24T05:51:17.177288Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Embeddings saved successfully!\n"]}],"source":["torch.save(train_embeddings, \"/kaggle/working/train_embeddings.pt\")\n","torch.save(val_embeddings, \"/kaggle/working/val_embeddings.pt\")\n","torch.save(test_embeddings, \"/kaggle/working/test_embeddings.pt\")\n","\n","print(\"Embeddings saved successfully!\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T05:52:09.139559Z","iopub.status.busy":"2024-11-24T05:52:09.138876Z","iopub.status.idle":"2024-11-24T05:52:09.763001Z","shell.execute_reply":"2024-11-24T05:52:09.762137Z","shell.execute_reply.started":"2024-11-24T05:52:09.139524Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/275305547.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_embeddings = torch.load(\"/kaggle/working/train_embeddings.pt\")\n"]},{"name":"stdout","output_type":"stream","text":["Train: 3013, Validation: 177, Test: 354\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/275305547.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_embeddings = torch.load(\"/kaggle/working/val_embeddings.pt\")\n","/tmp/ipykernel_30/275305547.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  test_embeddings = torch.load(\"/kaggle/working/test_embeddings.pt\")\n"]}],"source":["train_embeddings = torch.load(\"/kaggle/working/train_embeddings.pt\")\n","val_embeddings = torch.load(\"/kaggle/working/val_embeddings.pt\")\n","test_embeddings = torch.load(\"/kaggle/working/test_embeddings.pt\")\n","\n","print(f\"Train: {len(train_embeddings)}, Validation: {len(val_embeddings)}, Test: {len(test_embeddings)}\")"]},{"cell_type":"markdown","metadata":{},"source":["MAX_MODEL = IMAGE + TEXT + CAPTION + VQA"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T05:56:01.576205Z","iopub.status.busy":"2024-11-24T05:56:01.575857Z","iopub.status.idle":"2024-11-24T05:56:02.038349Z","shell.execute_reply":"2024-11-24T05:56:02.037421Z","shell.execute_reply.started":"2024-11-24T05:56:01.576174Z"},"trusted":true},"outputs":[],"source":["from torch_geometric.data import Data\n","\n","def create_graphs_from_embeddings(embeddings):\n","    graphs = []\n","    for embed in embeddings:\n","        \n","        node_features = torch.stack(\n","            [embed[\"image_embedding\"], embed[\"caption_embedding\"], embed[\"meme_text_embedding\"]] + list(embed[\"vqa_embeddings\"])\n","        )  # [num_nodes, feature_dim]\n","\n","        \n","        num_nodes = node_features.size(0)\n","        edge_indices = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","\n","        \n","        label = torch.tensor(embed[\"label\"], dtype=torch.long)\n","        graph = Data(x=node_features, edge_index=edge_index, y=label)\n","        graphs.append(graph)\n","    return graphs\n","\n","\n","train_graphs = create_graphs_from_embeddings(train_embeddings)\n","val_graphs = create_graphs_from_embeddings(val_embeddings)\n","test_graphs = create_graphs_from_embeddings(test_embeddings)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T05:56:47.049177Z","iopub.status.busy":"2024-11-24T05:56:47.048859Z","iopub.status.idle":"2024-11-24T05:56:47.054027Z","shell.execute_reply":"2024-11-24T05:56:47.053121Z","shell.execute_reply.started":"2024-11-24T05:56:47.049150Z"},"trusted":true},"outputs":[],"source":["from torch_geometric.loader import DataLoader\n","\n","\n","batch_size = 32\n","train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T05:58:40.602300Z","iopub.status.busy":"2024-11-24T05:58:40.601952Z","iopub.status.idle":"2024-11-24T05:58:46.176721Z","shell.execute_reply":"2024-11-24T05:58:46.175707Z","shell.execute_reply.started":"2024-11-24T05:58:40.602264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total trainable parameters in the model: 799,042\n","Starting training...\n","Epoch 1/50, Train Loss: 0.5033, Validation Loss: 0.4206, Validation Accuracy: 0.8362\n","Epoch 2/50, Train Loss: 0.4333, Validation Loss: 0.5028, Validation Accuracy: 0.7458\n","Epoch 3/50, Train Loss: 0.3908, Validation Loss: 0.5059, Validation Accuracy: 0.7345\n","Epoch 4/50, Train Loss: 0.3517, Validation Loss: 0.4698, Validation Accuracy: 0.7853\n","Epoch 5/50, Train Loss: 0.3296, Validation Loss: 0.4883, Validation Accuracy: 0.8192\n","Epoch 6/50, Train Loss: 0.2863, Validation Loss: 0.6922, Validation Accuracy: 0.7853\n","Early stopping at epoch 6\n","Training complete.\n","Starting testing...\n","Test Accuracy: 0.8503\n","Confusion Matrix:\n","[[204  26]\n"," [ 27  97]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/4076213365.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"best_model.pt\"))\n"]}],"source":["from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import GATConv, global_mean_pool\n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear\n","from torch.optim import Adam\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import numpy as np\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Define the Enhanced GAT model\n","class EnhancedGAT(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=4, dropout=0.5):\n","        super(EnhancedGAT, self).__init__()\n","        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads, concat=True)\n","        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=True)\n","        self.conv3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=False)\n","        self.fc1 = Linear(hidden_dim, hidden_dim // 2)\n","        self.fc2 = Linear(hidden_dim // 2, output_dim)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        if x.dim() == 3:\n","            x = x.squeeze(1)\n","        x = F.elu(self.conv1(x, edge_index))\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = F.elu(self.conv2(x, edge_index))\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = F.elu(self.conv3(x, edge_index))\n","        x = global_mean_pool(x, batch)  # Graph-level pooling\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","input_dim = graphs[0].x.size(-1)\n","hidden_dim = 128\n","output_dim = 2  # Binary classification\n","model = EnhancedGAT(input_dim, hidden_dim, output_dim).to(device)\n","print(f\"Total trainable parameters in the model: {count_parameters(model):,}\")\n","\n","optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","\n","best_val_loss = float('inf')\n","patience = 5\n","early_stop_counter = 0\n","\n","# Training and validation loop\n","epochs = 50\n","print(\"Starting training...\")\n","for epoch in range(epochs):\n","    # Training phase\n","    model.train()\n","    train_loss = 0\n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        optimizer.zero_grad()\n","        out = model(batch)\n","        loss = criterion(out, batch.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    train_loss /= len(train_loader)\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss = 0\n","    val_preds, val_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            batch = batch.to(device)\n","            if batch.x.dim() == 3:\n","                batch.x = batch.x.squeeze(1)\n","            out = model(batch)\n","            loss = criterion(out, batch.y)\n","            val_loss += loss.item()\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            labels = batch.y.cpu().numpy()\n","            val_preds.extend(preds)\n","            val_labels.extend(labels)\n","    val_loss /= len(val_loader)\n","    val_accuracy = accuracy_score(val_labels, val_preds)\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Early stopping logic\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stop_counter = 0\n","        torch.save(model.state_dict(), \"best_model.pt\")\n","    else:\n","        early_stop_counter += 1\n","        if early_stop_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    scheduler.step()\n","\n","print(\"Training complete.\")\n","\n","# Test phase\n","print(\"Starting testing...\")\n","model.load_state_dict(torch.load(\"best_model.pt\"))\n","model.eval()\n","test_preds, test_labels = [], []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        out = model(batch)\n","        preds = out.argmax(dim=1).cpu().numpy()\n","        labels = batch.y.cpu().numpy()\n","        test_preds.extend(preds)\n","        test_labels.extend(labels)\n","\n","test_accuracy = accuracy_score(test_labels, test_preds)\n","conf_matrix = confusion_matrix(test_labels, test_preds)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)"]},{"cell_type":"markdown","metadata":{},"source":["\bWithout VQA"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T06:17:56.530313Z","iopub.status.busy":"2024-11-24T06:17:56.529985Z","iopub.status.idle":"2024-11-24T06:18:02.218981Z","shell.execute_reply":"2024-11-24T06:18:02.218118Z","shell.execute_reply.started":"2024-11-24T06:17:56.530280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training without VQA...\n","Epoch 1/50, Train Loss: 0.4842, Validation Loss: 0.4364, Validation Accuracy: 0.7627\n","Epoch 2/50, Train Loss: 0.4095, Validation Loss: 0.5545, Validation Accuracy: 0.7853\n","Epoch 3/50, Train Loss: 0.3770, Validation Loss: 0.5801, Validation Accuracy: 0.7401\n","Epoch 4/50, Train Loss: 0.3423, Validation Loss: 0.5402, Validation Accuracy: 0.8136\n","Epoch 5/50, Train Loss: 0.3164, Validation Loss: 0.5519, Validation Accuracy: 0.7571\n","Epoch 6/50, Train Loss: 0.2868, Validation Loss: 0.5775, Validation Accuracy: 0.7571\n","Early stopping at epoch 6\n","Training without VQA complete.\n","Starting testing without VQA...\n","Test Accuracy without VQA: 0.8503\n","Confusion Matrix without VQA:\n","[[192  38]\n"," [ 15 109]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1336419865.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_no_vqa.load_state_dict(torch.load(\"best_model_no_vqa.pt\"))\n"]}],"source":["from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import GATConv, global_mean_pool\n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear\n","from torch.optim import Adam\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","\n","def create_graphs_without_vqa(embeddings):\n","    graphs = []\n","    for embed in embeddings:\n","        \n","        node_features = torch.stack(\n","            [embed[\"image_embedding\"], embed[\"caption_embedding\"], embed[\"meme_text_embedding\"]]\n","        )  # [num_nodes, feature_dim]\n","\n","        # 완전 연결 엣지 생성\n","        num_nodes = node_features.size(0)\n","        edge_indices = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","\n","        # 그래프 객체 생성\n","        label = torch.tensor(embed[\"label\"], dtype=torch.long)\n","        graph = Data(x=node_features, edge_index=edge_index, y=label)\n","        graphs.append(graph)\n","    return graphs\n","\n","# Train, Validation, Test 그래프 생성\n","train_graphs_no_vqa = create_graphs_without_vqa(train_embeddings)\n","val_graphs_no_vqa = create_graphs_without_vqa(val_embeddings)\n","test_graphs_no_vqa = create_graphs_without_vqa(test_embeddings)\n","\n","# 데이터 로더 생성\n","batch_size = 32\n","train_loader_no_vqa = DataLoader(train_graphs_no_vqa, batch_size=batch_size, shuffle=True)\n","val_loader_no_vqa = DataLoader(val_graphs_no_vqa, batch_size=batch_size, shuffle=False)\n","test_loader_no_vqa = DataLoader(test_graphs_no_vqa, batch_size=batch_size, shuffle=False)\n","\n","# Define Enhanced GAT model\n","class EnhancedGAT(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=4, dropout=0.5):\n","        super(EnhancedGAT, self).__init__()\n","        self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads, concat=True)\n","        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=True)\n","        self.conv3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=False)\n","        self.fc1 = Linear(hidden_dim, hidden_dim // 2)\n","        self.fc2 = Linear(hidden_dim // 2, output_dim)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        if x.dim() == 3:\n","            x = x.squeeze(1)\n","        x = F.elu(self.conv1(x, edge_index))\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = F.elu(self.conv2(x, edge_index))\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = F.elu(self.conv3(x, edge_index))\n","        x = global_mean_pool(x, batch)  # Graph-level pooling\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","# Model, optimizer, and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","input_dim_no_vqa = train_graphs_no_vqa[0].x.size(-1)\n","hidden_dim = 128\n","output_dim = 2  # Binary classification\n","model_no_vqa = EnhancedGAT(input_dim_no_vqa, hidden_dim, output_dim).to(device)\n","optimizer = Adam(model_no_vqa.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","# Early stopping 설정\n","best_val_loss_no_vqa = float('inf')\n","early_stop_counter_no_vqa = 0\n","patience = 5\n","\n","# Training and validation loop\n","epochs = 50\n","print(\"Starting training without VQA...\")\n","for epoch in range(epochs):\n","    # Training phase\n","    model_no_vqa.train()\n","    train_loss_no_vqa = 0\n","    for batch in train_loader_no_vqa:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        optimizer.zero_grad()\n","        out = model_no_vqa(batch)\n","        loss = criterion(out, batch.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss_no_vqa += loss.item()\n","    train_loss_no_vqa /= len(train_loader_no_vqa)\n","\n","    # Validation phase\n","    model_no_vqa.eval()\n","    val_loss_no_vqa = 0\n","    val_preds_no_vqa, val_labels_no_vqa = [], []\n","    with torch.no_grad():\n","        for batch in val_loader_no_vqa:\n","            batch = batch.to(device)\n","            if batch.x.dim() == 3:\n","                batch.x = batch.x.squeeze(1)\n","            out = model_no_vqa(batch)\n","            loss = criterion(out, batch.y)\n","            val_loss_no_vqa += loss.item()\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            labels = batch.y.cpu().numpy()\n","            val_preds_no_vqa.extend(preds)\n","            val_labels_no_vqa.extend(labels)\n","    val_loss_no_vqa /= len(val_loader_no_vqa)\n","    val_accuracy_no_vqa = accuracy_score(val_labels_no_vqa, val_preds_no_vqa)\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss_no_vqa:.4f}, Validation Loss: {val_loss_no_vqa:.4f}, Validation Accuracy: {val_accuracy_no_vqa:.4f}\")\n","\n","    # Early stopping logic\n","    if val_loss_no_vqa < best_val_loss_no_vqa:\n","        best_val_loss_no_vqa = val_loss_no_vqa\n","        early_stop_counter_no_vqa = 0\n","        torch.save(model_no_vqa.state_dict(), \"best_model_no_vqa.pt\")\n","    else:\n","        early_stop_counter_no_vqa += 1\n","        if early_stop_counter_no_vqa >= patience:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    scheduler.step()\n","\n","print(\"Training without VQA complete.\")\n","\n","# Test phase\n","print(\"Starting testing without VQA...\")\n","model_no_vqa.load_state_dict(torch.load(\"best_model_no_vqa.pt\"))\n","model_no_vqa.eval()\n","test_preds_no_vqa, test_labels_no_vqa = [], []\n","with torch.no_grad():\n","    for batch in test_loader_no_vqa:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        out = model_no_vqa(batch)\n","        preds = out.argmax(dim=1).cpu().numpy()\n","        labels = batch.y.cpu().numpy()\n","        test_preds_no_vqa.extend(preds)\n","        test_labels_no_vqa.extend(labels)\n","\n","test_accuracy_no_vqa = accuracy_score(test_labels_no_vqa, test_preds_no_vqa)\n","conf_matrix_no_vqa = confusion_matrix(test_labels_no_vqa, test_preds_no_vqa)\n","print(f\"Test Accuracy without VQA: {test_accuracy_no_vqa:.4f}\")\n","print(\"Confusion Matrix without VQA:\")\n","print(conf_matrix_no_vqa)"]},{"cell_type":"markdown","metadata":{},"source":["Without Captioning(BEST MODEL)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T06:09:09.287723Z","iopub.status.busy":"2024-11-24T06:09:09.287096Z","iopub.status.idle":"2024-11-24T06:09:15.368962Z","shell.execute_reply":"2024-11-24T06:09:15.368014Z","shell.execute_reply.started":"2024-11-24T06:09:09.287687Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training without Captioning...\n","Epoch 1/50, Train Loss: 0.5061, Validation Loss: 0.4751, Validation Accuracy: 0.8023\n","Epoch 2/50, Train Loss: 0.4165, Validation Loss: 0.5448, Validation Accuracy: 0.7571\n","Epoch 3/50, Train Loss: 0.3877, Validation Loss: 0.5772, Validation Accuracy: 0.7571\n","Epoch 4/50, Train Loss: 0.3537, Validation Loss: 0.5575, Validation Accuracy: 0.7288\n","Epoch 5/50, Train Loss: 0.3133, Validation Loss: 0.5660, Validation Accuracy: 0.7401\n","Epoch 6/50, Train Loss: 0.2994, Validation Loss: 0.6502, Validation Accuracy: 0.7514\n","Early stopping at epoch 6\n","Training without Captioning complete.\n","Starting testing without Captioning...\n","Test Accuracy without Captioning: 0.8729\n","Confusion Matrix without Captioning:\n","[[202  28]\n"," [ 17 107]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/2051731954.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_no_caption.load_state_dict(torch.load(\"best_model_no_caption.pt\"))\n"]}],"source":["from torch_geometric.data import Data\n","\n","\n","def create_graphs_without_captioning(embeddings):\n","    graphs = []\n","    for embed in embeddings:\n","        \n","        node_features = torch.stack(\n","            [embed[\"image_embedding\"], embed[\"meme_text_embedding\"]] + list(embed[\"vqa_embeddings\"])\n","        )  # [num_nodes, feature_dim]\n","\n","        \n","        num_nodes = node_features.size(0)\n","        edge_indices = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","\n","        \n","        label = torch.tensor(embed[\"label\"], dtype=torch.long)\n","        graph = Data(x=node_features, edge_index=edge_index, y=label)\n","        graphs.append(graph)\n","    return graphs\n","\n","\n","train_graphs_no_caption = create_graphs_without_captioning(train_embeddings)\n","val_graphs_no_caption = create_graphs_without_captioning(val_embeddings)\n","test_graphs_no_caption = create_graphs_without_captioning(test_embeddings)\n","\n","\n","batch_size = 32\n","train_loader_no_caption = DataLoader(train_graphs_no_caption, batch_size=batch_size, shuffle=True)\n","val_loader_no_caption = DataLoader(val_graphs_no_caption, batch_size=batch_size, shuffle=False)\n","test_loader_no_caption = DataLoader(test_graphs_no_caption, batch_size=batch_size, shuffle=False)\n","\n","\n","input_dim_no_caption = train_graphs_no_caption[0].x.size(-1)  # 노드 특징 크기\n","model_no_caption = EnhancedGAT(input_dim_no_caption, hidden_dim, output_dim).to(device)\n","\n","optimizer = Adam(model_no_caption.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","\n","best_val_loss_no_caption = float('inf')\n","early_stop_counter_no_caption = 0\n","\n","# Training and validation loop\n","print(\"Starting training without Captioning...\")\n","for epoch in range(epochs):\n","    # Training phase\n","    model_no_caption.train()\n","    train_loss_no_caption = 0\n","    for batch in train_loader_no_caption:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        optimizer.zero_grad()\n","        out = model_no_caption(batch)\n","        loss = criterion(out, batch.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss_no_caption += loss.item()\n","    train_loss_no_caption /= len(train_loader_no_caption)\n","\n","    # Validation phase\n","    model_no_caption.eval()\n","    val_loss_no_caption = 0\n","    val_preds_no_caption, val_labels_no_caption = [], []\n","    with torch.no_grad():\n","        for batch in val_loader_no_caption:\n","            batch = batch.to(device)\n","            if batch.x.dim() == 3:\n","                batch.x = batch.x.squeeze(1)\n","            out = model_no_caption(batch)\n","            loss = criterion(out, batch.y)\n","            val_loss_no_caption += loss.item()\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            labels = batch.y.cpu().numpy()\n","            val_preds_no_caption.extend(preds)\n","            val_labels_no_caption.extend(labels)\n","    val_loss_no_caption /= len(val_loader_no_caption)\n","    val_accuracy_no_caption = accuracy_score(val_labels_no_caption, val_preds_no_caption)\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss_no_caption:.4f}, Validation Loss: {val_loss_no_caption:.4f}, Validation Accuracy: {val_accuracy_no_caption:.4f}\")\n","\n","    # Early stopping logic\n","    if val_loss_no_caption < best_val_loss_no_caption:\n","        best_val_loss_no_caption = val_loss_no_caption\n","        early_stop_counter_no_caption = 0\n","        torch.save(model_no_caption.state_dict(), \"best_model_no_caption.pt\")\n","    else:\n","        early_stop_counter_no_caption += 1\n","        if early_stop_counter_no_caption >= patience:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    scheduler.step()\n","\n","print(\"Training without Captioning complete.\")\n","\n","# Test phase\n","print(\"Starting testing without Captioning...\")\n","model_no_caption.load_state_dict(torch.load(\"best_model_no_caption.pt\"))\n","model_no_caption.eval()\n","test_preds_no_caption, test_labels_no_caption = [], []\n","with torch.no_grad():\n","    for batch in test_loader_no_caption:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        out = model_no_caption(batch)\n","        preds = out.argmax(dim=1).cpu().numpy()\n","        labels = batch.y.cpu().numpy()\n","        test_preds_no_caption.extend(preds)\n","        test_labels_no_caption.extend(labels)\n","\n","test_accuracy_no_caption = accuracy_score(test_labels_no_caption, test_preds_no_caption)\n","conf_matrix_no_caption = confusion_matrix(test_labels_no_caption, test_preds_no_caption)\n","print(f\"Test Accuracy without Captioning: {test_accuracy_no_caption:.4f}\")\n","print(\"Confusion Matrix without Captioning:\")\n","print(conf_matrix_no_caption)"]},{"cell_type":"markdown","metadata":{},"source":["Without Image"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T06:10:29.301455Z","iopub.status.busy":"2024-11-24T06:10:29.301095Z","iopub.status.idle":"2024-11-24T06:10:35.281154Z","shell.execute_reply":"2024-11-24T06:10:35.280384Z","shell.execute_reply.started":"2024-11-24T06:10:29.301422Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training without Image...\n","Epoch 1/50, Train Loss: 0.5301, Validation Loss: 0.4867, Validation Accuracy: 0.7684\n","Epoch 2/50, Train Loss: 0.4483, Validation Loss: 0.5085, Validation Accuracy: 0.7232\n","Epoch 3/50, Train Loss: 0.3979, Validation Loss: 0.5315, Validation Accuracy: 0.7740\n","Epoch 4/50, Train Loss: 0.3803, Validation Loss: 0.7081, Validation Accuracy: 0.7175\n","Epoch 5/50, Train Loss: 0.3467, Validation Loss: 0.5962, Validation Accuracy: 0.7401\n","Epoch 6/50, Train Loss: 0.3330, Validation Loss: 0.7158, Validation Accuracy: 0.7345\n","Early stopping at epoch 6\n","Training without Image complete.\n","Starting testing without Image...\n","Test Accuracy without Image: 0.8277\n","Confusion Matrix without Image:\n","[[194  36]\n"," [ 25  99]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/3822031046.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_no_image.load_state_dict(torch.load(\"best_model_no_image.pt\"))\n"]}],"source":["from torch_geometric.data import Data\n","\n","\n","def create_graphs_without_image(embeddings):\n","    graphs = []\n","    for embed in embeddings:\n","        \n","        node_features = torch.stack(\n","            [embed[\"caption_embedding\"], embed[\"meme_text_embedding\"]] + list(embed[\"vqa_embeddings\"])\n","        )  # [num_nodes, feature_dim]\n","\n","        \n","        num_nodes = node_features.size(0)\n","        edge_indices = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","\n","        \n","        label = torch.tensor(embed[\"label\"], dtype=torch.long)\n","        graph = Data(x=node_features, edge_index=edge_index, y=label)\n","        graphs.append(graph)\n","    return graphs\n","\n","\n","train_graphs_no_image = create_graphs_without_image(train_embeddings)\n","val_graphs_no_image = create_graphs_without_image(val_embeddings)\n","test_graphs_no_image = create_graphs_without_image(test_embeddings)\n","\n","\n","batch_size = 32\n","train_loader_no_image = DataLoader(train_graphs_no_image, batch_size=batch_size, shuffle=True)\n","val_loader_no_image = DataLoader(val_graphs_no_image, batch_size=batch_size, shuffle=False)\n","test_loader_no_image = DataLoader(test_graphs_no_image, batch_size=batch_size, shuffle=False)\n","\n","\n","input_dim_no_image = train_graphs_no_image[0].x.size(-1)  \n","model_no_image = EnhancedGAT(input_dim_no_image, hidden_dim, output_dim).to(device)\n","\n","optimizer = Adam(model_no_image.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","\n","best_val_loss_no_image = float('inf')\n","early_stop_counter_no_image = 0\n","\n","# Training and validation loop\n","print(\"Starting training without Image...\")\n","for epoch in range(epochs):\n","    # Training phase\n","    model_no_image.train()\n","    train_loss_no_image = 0\n","    for batch in train_loader_no_image:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        optimizer.zero_grad()\n","        out = model_no_image(batch)\n","        loss = criterion(out, batch.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss_no_image += loss.item()\n","    train_loss_no_image /= len(train_loader_no_image)\n","\n","    # Validation phase\n","    model_no_image.eval()\n","    val_loss_no_image = 0\n","    val_preds_no_image, val_labels_no_image = [], []\n","    with torch.no_grad():\n","        for batch in val_loader_no_image:\n","            batch = batch.to(device)\n","            if batch.x.dim() == 3:\n","                batch.x = batch.x.squeeze(1)\n","            out = model_no_image(batch)\n","            loss = criterion(out, batch.y)\n","            val_loss_no_image += loss.item()\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            labels = batch.y.cpu().numpy()\n","            val_preds_no_image.extend(preds)\n","            val_labels_no_image.extend(labels)\n","    val_loss_no_image /= len(val_loader_no_image)\n","    val_accuracy_no_image = accuracy_score(val_labels_no_image, val_preds_no_image)\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss_no_image:.4f}, Validation Loss: {val_loss_no_image:.4f}, Validation Accuracy: {val_accuracy_no_image:.4f}\")\n","\n","    # Early stopping logic\n","    if val_loss_no_image < best_val_loss_no_image:\n","        best_val_loss_no_image = val_loss_no_image\n","        early_stop_counter_no_image = 0\n","        torch.save(model_no_image.state_dict(), \"best_model_no_image.pt\")\n","    else:\n","        early_stop_counter_no_image += 1\n","        if early_stop_counter_no_image >= patience:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    scheduler.step()\n","\n","print(\"Training without Image complete.\")\n","\n","# Test phase\n","print(\"Starting testing without Image...\")\n","model_no_image.load_state_dict(torch.load(\"best_model_no_image.pt\"))\n","model_no_image.eval()\n","test_preds_no_image, test_labels_no_image = [], []\n","with torch.no_grad():\n","    for batch in test_loader_no_image:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        out = model_no_image(batch)\n","        preds = out.argmax(dim=1).cpu().numpy()\n","        labels = batch.y.cpu().numpy()\n","        test_preds_no_image.extend(preds)\n","        test_labels_no_image.extend(labels)\n","\n","test_accuracy_no_image = accuracy_score(test_labels_no_image, test_preds_no_image)\n","conf_matrix_no_image = confusion_matrix(test_labels_no_image, test_preds_no_image)\n","print(f\"Test Accuracy without Image: {test_accuracy_no_image:.4f}\")\n","print(\"Confusion Matrix without Image:\")\n","print(conf_matrix_no_image)"]},{"cell_type":"markdown","metadata":{},"source":["Without Meme TEXT"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T06:11:13.496017Z","iopub.status.busy":"2024-11-24T06:11:13.495697Z","iopub.status.idle":"2024-11-24T06:11:20.458046Z","shell.execute_reply":"2024-11-24T06:11:20.457147Z","shell.execute_reply.started":"2024-11-24T06:11:13.495992Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training without Text...\n","Epoch 1/50, Train Loss: 0.5120, Validation Loss: 0.5899, Validation Accuracy: 0.7458\n","Epoch 2/50, Train Loss: 0.4507, Validation Loss: 0.4952, Validation Accuracy: 0.7684\n","Epoch 3/50, Train Loss: 0.4001, Validation Loss: 0.6357, Validation Accuracy: 0.7345\n","Epoch 4/50, Train Loss: 0.3720, Validation Loss: 0.5893, Validation Accuracy: 0.7627\n","Epoch 5/50, Train Loss: 0.3413, Validation Loss: 0.5574, Validation Accuracy: 0.7571\n","Epoch 6/50, Train Loss: 0.3247, Validation Loss: 0.6544, Validation Accuracy: 0.7458\n","Epoch 7/50, Train Loss: 0.2921, Validation Loss: 0.6084, Validation Accuracy: 0.7345\n","Early stopping at epoch 7\n","Training without Text complete.\n","Starting testing without Text...\n","Test Accuracy without Text: 0.8362\n","Confusion Matrix without Text:\n","[[194  36]\n"," [ 22 102]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/4144254214.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_no_text.load_state_dict(torch.load(\"best_model_no_text.pt\"))\n"]}],"source":["from torch_geometric.data import Data\n","\n","\n","def create_graphs_without_text(embeddings):\n","    graphs = []\n","    for embed in embeddings:\n","        \n","        node_features = torch.stack(\n","            [embed[\"image_embedding\"], embed[\"caption_embedding\"]] + list(embed[\"vqa_embeddings\"])\n","        )  # [num_nodes, feature_dim]\n","\n","        \n","        num_nodes = node_features.size(0)\n","        edge_indices = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","\n","        \n","        label = torch.tensor(embed[\"label\"], dtype=torch.long)\n","        graph = Data(x=node_features, edge_index=edge_index, y=label)\n","        graphs.append(graph)\n","    return graphs\n","\n","\n","train_graphs_no_text = create_graphs_without_text(train_embeddings)\n","val_graphs_no_text = create_graphs_without_text(val_embeddings)\n","test_graphs_no_text = create_graphs_without_text(test_embeddings)\n","\n","\n","batch_size = 32\n","train_loader_no_text = DataLoader(train_graphs_no_text, batch_size=batch_size, shuffle=True)\n","val_loader_no_text = DataLoader(val_graphs_no_text, batch_size=batch_size, shuffle=False)\n","test_loader_no_text = DataLoader(test_graphs_no_text, batch_size=batch_size, shuffle=False)\n","\n","\n","input_dim_no_text = train_graphs_no_text[0].x.size(-1)  \n","model_no_text = EnhancedGAT(input_dim_no_text, hidden_dim, output_dim).to(device)\n","\n","optimizer = Adam(model_no_text.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","\n","best_val_loss_no_text = float('inf')\n","early_stop_counter_no_text = 0\n","\n","# Training and validation loop\n","print(\"Starting training without Text...\")\n","for epoch in range(epochs):\n","    # Training phase\n","    model_no_text.train()\n","    train_loss_no_text = 0\n","    for batch in train_loader_no_text:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        optimizer.zero_grad()\n","        out = model_no_text(batch)\n","        loss = criterion(out, batch.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss_no_text += loss.item()\n","    train_loss_no_text /= len(train_loader_no_text)\n","\n","    # Validation phase\n","    model_no_text.eval()\n","    val_loss_no_text = 0\n","    val_preds_no_text, val_labels_no_text = [], []\n","    with torch.no_grad():\n","        for batch in val_loader_no_text:\n","            batch = batch.to(device)\n","            if batch.x.dim() == 3:\n","                batch.x = batch.x.squeeze(1)\n","            out = model_no_text(batch)\n","            loss = criterion(out, batch.y)\n","            val_loss_no_text += loss.item()\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            labels = batch.y.cpu().numpy()\n","            val_preds_no_text.extend(preds)\n","            val_labels_no_text.extend(labels)\n","    val_loss_no_text /= len(val_loader_no_text)\n","    val_accuracy_no_text = accuracy_score(val_labels_no_text, val_preds_no_text)\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss_no_text:.4f}, Validation Loss: {val_loss_no_text:.4f}, Validation Accuracy: {val_accuracy_no_text:.4f}\")\n","\n","    # Early stopping logic\n","    if val_loss_no_text < best_val_loss_no_text:\n","        best_val_loss_no_text = val_loss_no_text\n","        early_stop_counter_no_text = 0\n","        torch.save(model_no_text.state_dict(), \"best_model_no_text.pt\")\n","    else:\n","        early_stop_counter_no_text += 1\n","        if early_stop_counter_no_text >= patience:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    scheduler.step()\n","\n","print(\"Training without Text complete.\")\n","\n","# Test phase\n","print(\"Starting testing without Text...\")\n","model_no_text.load_state_dict(torch.load(\"best_model_no_text.pt\"))\n","model_no_text.eval()\n","test_preds_no_text, test_labels_no_text = [], []\n","with torch.no_grad():\n","    for batch in test_loader_no_text:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        out = model_no_text(batch)\n","        preds = out.argmax(dim=1).cpu().numpy()\n","        labels = batch.y.cpu().numpy()\n","        test_preds_no_text.extend(preds)\n","        test_labels_no_text.extend(labels)\n","\n","test_accuracy_no_text = accuracy_score(test_labels_no_text, test_preds_no_text)\n","conf_matrix_no_text = confusion_matrix(test_labels_no_text, test_preds_no_text)\n","print(f\"Test Accuracy without Text: {test_accuracy_no_text:.4f}\")\n","print(\"Confusion Matrix without Text:\")\n","print(conf_matrix_no_text)"]},{"cell_type":"markdown","metadata":{},"source":["Without captioning and VQA(ONLY MEME TEXT, IMAGE)"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-11-24T06:12:50.157659Z","iopub.status.busy":"2024-11-24T06:12:50.157204Z","iopub.status.idle":"2024-11-24T06:12:55.685638Z","shell.execute_reply":"2024-11-24T06:12:55.684814Z","shell.execute_reply.started":"2024-11-24T06:12:50.157614Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training without Caption & VQA...\n","Epoch 1/50, Train Loss: 0.4715, Validation Loss: 0.5536, Validation Accuracy: 0.7627\n","Epoch 2/50, Train Loss: 0.4136, Validation Loss: 0.5747, Validation Accuracy: 0.7175\n","Epoch 3/50, Train Loss: 0.3911, Validation Loss: 0.6733, Validation Accuracy: 0.7062\n","Epoch 4/50, Train Loss: 0.3521, Validation Loss: 0.5845, Validation Accuracy: 0.7401\n","Epoch 5/50, Train Loss: 0.3266, Validation Loss: 0.6044, Validation Accuracy: 0.7458\n","Epoch 6/50, Train Loss: 0.2965, Validation Loss: 0.6521, Validation Accuracy: 0.6949\n","Early stopping at epoch 6\n","Training without Caption & VQA complete.\n","Starting testing without Caption & VQA...\n","Test Accuracy without Caption & VQA: 0.8531\n","Confusion Matrix without Caption & VQA:\n","[[193  37]\n"," [ 15 109]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1408557412.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_no_caption_vqa.load_state_dict(torch.load(\"best_model_no_caption_vqa.pt\"))\n"]}],"source":["from torch_geometric.data import Data\n","\n","\n","def create_graphs_without_caption_vqa(embeddings):\n","    graphs = []\n","    for embed in embeddings:\n","        \n","        node_features = torch.stack(\n","            [embed[\"image_embedding\"], embed[\"meme_text_embedding\"]]\n","        )  # [num_nodes, feature_dim]\n","\n","        \n","        num_nodes = node_features.size(0)\n","        edge_indices = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n","\n","        \n","        label = torch.tensor(embed[\"label\"], dtype=torch.long)\n","        graph = Data(x=node_features, edge_index=edge_index, y=label)\n","        graphs.append(graph)\n","    return graphs\n","\n","# Train, Validation, Test \n","train_graphs_no_caption_vqa = create_graphs_without_caption_vqa(train_embeddings)\n","val_graphs_no_caption_vqa = create_graphs_without_caption_vqa(val_embeddings)\n","test_graphs_no_caption_vqa = create_graphs_without_caption_vqa(test_embeddings)\n","\n","\n","batch_size = 32\n","train_loader_no_caption_vqa = DataLoader(train_graphs_no_caption_vqa, batch_size=batch_size, shuffle=True)\n","val_loader_no_caption_vqa = DataLoader(val_graphs_no_caption_vqa, batch_size=batch_size, shuffle=False)\n","test_loader_no_caption_vqa = DataLoader(test_graphs_no_caption_vqa, batch_size=batch_size, shuffle=False)\n","\n","\n","input_dim_no_caption_vqa = train_graphs_no_caption_vqa[0].x.size(-1)  \n","model_no_caption_vqa = EnhancedGAT(input_dim_no_caption_vqa, hidden_dim, output_dim).to(device)\n","\n","optimizer = Adam(model_no_caption_vqa.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","\n","best_val_loss_no_caption_vqa = float('inf')\n","early_stop_counter_no_caption_vqa = 0\n","\n","# Training and validation loop\n","print(\"Starting training without Caption & VQA...\")\n","for epoch in range(epochs):\n","    # Training phase\n","    model_no_caption_vqa.train()\n","    train_loss_no_caption_vqa = 0\n","    for batch in train_loader_no_caption_vqa:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        optimizer.zero_grad()\n","        out = model_no_caption_vqa(batch)\n","        loss = criterion(out, batch.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss_no_caption_vqa += loss.item()\n","    train_loss_no_caption_vqa /= len(train_loader_no_caption_vqa)\n","\n","    # Validation phase\n","    model_no_caption_vqa.eval()\n","    val_loss_no_caption_vqa = 0\n","    val_preds_no_caption_vqa, val_labels_no_caption_vqa = [], []\n","    with torch.no_grad():\n","        for batch in val_loader_no_caption_vqa:\n","            batch = batch.to(device)\n","            if batch.x.dim() == 3:\n","                batch.x = batch.x.squeeze(1)\n","            out = model_no_caption_vqa(batch)\n","            loss = criterion(out, batch.y)\n","            val_loss_no_caption_vqa += loss.item()\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            labels = batch.y.cpu().numpy()\n","            val_preds_no_caption_vqa.extend(preds)\n","            val_labels_no_caption_vqa.extend(labels)\n","    val_loss_no_caption_vqa /= len(val_loader_no_caption_vqa)\n","    val_accuracy_no_caption_vqa = accuracy_score(val_labels_no_caption_vqa, val_preds_no_caption_vqa)\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss_no_caption_vqa:.4f}, Validation Loss: {val_loss_no_caption_vqa:.4f}, Validation Accuracy: {val_accuracy_no_caption_vqa:.4f}\")\n","\n","    # Early stopping logic\n","    if val_loss_no_caption_vqa < best_val_loss_no_caption_vqa:\n","        best_val_loss_no_caption_vqa = val_loss_no_caption_vqa\n","        early_stop_counter_no_caption_vqa = 0\n","        torch.save(model_no_caption_vqa.state_dict(), \"best_model_no_caption_vqa.pt\")\n","    else:\n","        early_stop_counter_no_caption_vqa += 1\n","        if early_stop_counter_no_caption_vqa >= patience:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    scheduler.step()\n","\n","print(\"Training without Caption & VQA complete.\")\n","\n","# Test phase\n","print(\"Starting testing without Caption & VQA...\")\n","model_no_caption_vqa.load_state_dict(torch.load(\"best_model_no_caption_vqa.pt\"))\n","model_no_caption_vqa.eval()\n","test_preds_no_caption_vqa, test_labels_no_caption_vqa = [], []\n","with torch.no_grad():\n","    for batch in test_loader_no_caption_vqa:\n","        batch = batch.to(device)\n","        if batch.x.dim() == 3:\n","            batch.x = batch.x.squeeze(1)\n","        out = model_no_caption_vqa(batch)\n","        preds = out.argmax(dim=1).cpu().numpy()\n","        labels = batch.y.cpu().numpy()\n","        test_preds_no_caption_vqa.extend(preds)\n","        test_labels_no_caption_vqa.extend(labels)\n","\n","test_accuracy_no_caption_vqa = accuracy_score(test_labels_no_caption_vqa, test_preds_no_caption_vqa)\n","conf_matrix_no_caption_vqa = confusion_matrix(test_labels_no_caption_vqa, test_preds_no_caption_vqa)\n","print(f\"Test Accuracy without Caption & VQA: {test_accuracy_no_caption_vqa:.4f}\")\n","print(\"Confusion Matrix without Caption & VQA:\")\n","print(conf_matrix_no_caption_vqa)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5948939,"sourceId":9722793,"sourceType":"datasetVersion"},{"datasetId":6146148,"sourceId":9987058,"sourceType":"datasetVersion"},{"datasetId":6146651,"sourceId":9987839,"sourceType":"datasetVersion"},{"datasetId":6152531,"sourceId":9996311,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
